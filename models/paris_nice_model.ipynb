{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "paris_nice_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+TjUs79RASEToGJRkPe5U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pierre-brendan/PelotonIQ/blob/master/models/paris_nice_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBT1R8ObJoSU",
        "colab_type": "text"
      },
      "source": [
        "# First attempt at a model to predict the winner of Paris - Nice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSXKvTvZJjO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount drive to get cyclist data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tZ6OqLLJysB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All this code is from the Kmeans clustering code\n",
        "\n",
        "# load modules\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# load the rider data\n",
        "cycling_data = pd.read_csv('/content/drive/My Drive/cycling_data/rider_data.csv')\n",
        "cycling_data = cycling_data[cycling_data['Rider'] != 'Bjorg Lambrecht ']\n",
        "cycling_data = cycling_data[cycling_data['Rider'] != 'Bjorg Lambrecht']\n",
        "cycling_data = cycling_data[cycling_data['Rider'] != 'Robbert de Greef ']\n",
        "cycling_data = cycling_data[cycling_data['Rider'] != 'Robbert de Greef']\n",
        "cycling_data = cycling_data[cycling_data['Rider'] != 'Could not find rider']\n",
        "\n",
        "# Load the GC data\n",
        "gc_winners = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/gc_winners_consolidated.csv')\n",
        "gc_winners.rename(columns = {'Cyclist':'Rider'}, inplace = True)\n",
        "\n",
        "# Merge the GC winners to the rider data set\n",
        "cycling_data = pd.merge(cycling_data, gc_winners, how='left', on='Rider')\n",
        "\n",
        "# Load in the Monuments data\n",
        "san_remo = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/san_remo.csv')\n",
        "san_remo = san_remo['san_remo_win'].groupby(san_remo['Rider']).sum()\n",
        "roubaix = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/roubaix.csv')\n",
        "roubaix = roubaix['roubaix_win'].groupby(roubaix['Rider']).sum()\n",
        "flanders = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/flanders.csv')\n",
        "flanders = flanders['flanders_win'].groupby(flanders['Rider']).sum()\n",
        "lombardia = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/lombardia.csv')\n",
        "lombardia = lombardia['lombardia_win'].groupby(lombardia['Rider']).sum()\n",
        "liege = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/liege.csv')\n",
        "liege = liege['lbl_win'].groupby(liege['Rider']).sum()\n",
        "strade = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/strade.csv')\n",
        "strade['strade_win'] = 1\n",
        "strade = strade['strade_win'].groupby(strade['Rider']).sum()\n",
        "suisse = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/suisse.csv')\n",
        "suisse['suisse_win'] = 1\n",
        "suisse = suisse['suisse_win'].groupby(suisse['Rider']).sum()\n",
        "romandie = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/romandie.csv')\n",
        "romandie['romandie_win'] = 1\n",
        "romandie = romandie['romandie_win'].groupby(romandie['Rider']).sum()\n",
        "paris_nice = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/paris_nice.csv')\n",
        "paris_nice['paris_nice_win'] = 1\n",
        "paris_nice = paris_nice['paris_nice_win'].groupby(paris_nice['Rider']).sum()\n",
        "gent = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/gent.csv')\n",
        "gent['gent_win'] = 1\n",
        "gent = gent['gent_win'].groupby(gent['Rider']).sum()\n",
        "fleche = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/fleche.csv')\n",
        "fleche['fleche_win'] = 1\n",
        "fleche = fleche['fleche_win'].groupby(fleche['Rider']).sum()\n",
        "e3_binckbank = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/e3_binckbank.csv')\n",
        "e3_binckbank['e3_binckbank_win'] = 1\n",
        "e3_binckbank = e3_binckbank['e3_binckbank_win'].groupby(e3_binckbank['Rider']).sum()\n",
        "dauphine = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/dauphine.csv')\n",
        "dauphine['dauphine_win'] = 1\n",
        "dauphine = dauphine['dauphine_win'].groupby(dauphine['Rider']).sum()\n",
        "amstel = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/amstel.csv')\n",
        "amstel['amstel_win'] = 1\n",
        "amstel = amstel['amstel_win'].groupby(amstel['Rider']).sum()\n",
        "\n",
        "\n",
        "# Merge the Monuments winners to the rider data set\n",
        "cycling_data = pd.merge(cycling_data, san_remo, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, flanders, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, liege, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, lombardia, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, roubaix, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, strade, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, suisse, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, fleche, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, gent, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, paris_nice, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, e3_binckbank, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, amstel, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, dauphine, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, romandie, how='left', on='Rider')\n",
        "\n",
        "# Replace NaN with 0's\n",
        "cycling_data.fillna(0, inplace=True)\n",
        "\n",
        "# Let's weed out the lower tiered teams for now\n",
        "cycling_data['tmp_count'] = 1\n",
        "ccc = cycling_data['tmp_count'].groupby([cycling_data['Team']]).sum()\n",
        "cycling_data = cycling_data.drop(['tmp_count'], axis = 1)\n",
        "cycling_data = pd.merge(cycling_data, ccc, on='Team', how='left')\n",
        "cycling_data = cycling_data[cycling_data['tmp_count'] > 4]\n",
        "\n",
        "# Drop country variable\n",
        "cycling_data = cycling_data.drop(['Country', 'crawl_date', 'tmp_count'], axis = 1)\n",
        "#cycling_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNIWRKzCKPUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cycling_data.describe()\n",
        "# Okay, let's make a column for winners of Paris-Nice type races\n",
        "# Currently I'm thinking Paris - Nice, Tour de Suisse and Dauphine (Tierrento is not in the data set currently)\n",
        "cycling_data['Paris_Nice_etc'] = cycling_data['paris_nice_win'] + cycling_data['dauphine_win'] + cycling_data['suisse_win'] + cycling_data['romandie_win']\n",
        "\n",
        "# Fix the age column type\n",
        "cycling_data['Age'] = pd.to_numeric(cycling_data['Age'])\n",
        "\n",
        "# Will need to drop those 3 columns now\n",
        "cycling_data = cycling_data.drop(['paris_nice_win', 'dauphine_win', 'suisse_win', 'romandie_win'], axis = 1)\n",
        "\n",
        "# Split the data sets\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_set, test_set = train_test_split(cycling_data, test_size = 0.3, \n",
        "                                       random_state = 42)\n",
        "\n",
        "# first let's make a fresh copy of our data and drop what we are trying\n",
        "# to predict so it doesn't have any transmations applied to it\n",
        "psg = train_set.drop('Paris_Nice_etc', axis=1)\n",
        "psg_labels = train_set['Paris_Nice_etc'].copy()\n",
        "psg_rider = train_set['Rider'].copy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOcvD8lLKL4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Also from KMeans code\n",
        "# This would be the best approach to removing special characters\n",
        "#dictionary = {'ž':'z', 'č':'c', 'ü':'u', 'á':'a', 'é':'e', 'ö':'o', 'ó':'o', 'ç':'c',\n",
        "#              'ń':'ń', 'í':'i', 'è':'e'}\n",
        "#cycling_data.replace(dictionary, regex=True, inplace=True)\n",
        "\n",
        "#d = psg.drop(['Rider'], axis=1) #, 'Team'\n",
        "\n",
        "# let's normalize the data\n",
        "psg['GC'] = preprocessing.scale(psg['GC'])\n",
        "psg['Sprint'] = preprocessing.scale(psg['Sprint'])\n",
        "psg['TT'] = preprocessing.scale(psg['TT'])\n",
        "psg['Climber'] = preprocessing.scale(psg['Climber'])\n",
        "psg['Classic'] = preprocessing.scale(psg['Classic'])\n",
        "psg['Age'] = preprocessing.scale(psg['Age'])\n",
        "psg['Giro'] = preprocessing.scale(psg['Giro'])\n",
        "psg['Vuelta'] = preprocessing.scale(psg['Vuelta'])\n",
        "psg['Tour'] = preprocessing.scale(psg['Tour'])\n",
        "psg['Total'] = preprocessing.scale(psg['Total'])\n",
        "psg['san_remo_win'] = preprocessing.scale(psg['san_remo_win'])\n",
        "psg['lombardia_win'] = preprocessing.scale(psg['lombardia_win'])\n",
        "psg['flanders_win'] = preprocessing.scale(psg['flanders_win'])\n",
        "psg['lbl_win'] = preprocessing.scale(psg['lbl_win'])\n",
        "psg['roubaix_win'] = preprocessing.scale(psg['roubaix_win'])\n",
        "psg['strade_win'] = preprocessing.scale(psg['strade_win'])\n",
        "#psg['suisse_win'] = preprocessing.scale(psg['suisse_win'])\n",
        "psg['fleche_win'] = preprocessing.scale(psg['fleche_win'])\n",
        "psg['gent_win'] = preprocessing.scale(psg['gent_win'])\n",
        "#psg['Paris_Nice_etc'] = preprocessing.scale(psg['Paris_Nice_etc'])\n",
        "psg['e3_binckbank_win'] = preprocessing.scale(psg['e3_binckbank_win'])\n",
        "psg['amstel_win'] = preprocessing.scale(psg['amstel_win'])\n",
        "#psg['dauphine_win'] = preprocessing.scale(psg['dauphine_win'])\n",
        "#psg['romandie_win'] = preprocessing.scale(psg['romandie_win'])\n",
        "\n",
        "# Algorithims prefer working with numbers generally, so let's convert the team\n",
        "# proximity columns to numeric values\n",
        "psg['Team'] = psg['Team'].astype(str)\n",
        "housing_cat = psg[['Team']]\n",
        "# Problem is if we use numeric examples for these variables algorithims\n",
        "# will think they are near eachother. Instead we should make some one-hot\n",
        "# encodings otherwise known as dummy variables for each category as it's own\n",
        "# column\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "#ordinal_encoder = OrdinalEncoder()\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "#cat_encoder = OrdinalEncoder()\n",
        "#psg_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "#cat_attribs = ['Team']\n",
        "\n",
        "#full_pipeline = ColumnTransformer([\n",
        "#      ('cat', OneHotEncoder(), cat_attribs),\n",
        "#])\n",
        "\n",
        "#psg_prepared = full_pipeline.fit_transform(psg) # applying this to pandas dataframe called housing\n",
        "\n",
        "# not a fan of the approach above, changing it in the section below\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvBce0d5TC89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use pd.concat to join the new columns with your original dataframe\n",
        "psg = pd.concat([psg,pd.get_dummies(psg['Team'], prefix='Team')],axis=1)\n",
        "\n",
        "# now drop the original 'country' column (you don't need it anymore)\n",
        "psg.drop(['Team'],axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3qspSfrMwIY",
        "colab_type": "code",
        "outputId": "635c2ac3-0cb5-4a8a-9845-1bfe634b96f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "# Drop rider name\n",
        "psg = psg.drop(['Rider'], axis=1)\n",
        "\n",
        "# rename so rest of coding works\n",
        "psg_prepared = psg\n",
        "\n",
        "# Load this for RMSE calcs\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Linear regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(psg_prepared, psg_labels)\n",
        "\n",
        "# Let's test more powerful model\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "# Build decision tree model\n",
        "tree_reg = DecisionTreeRegressor()\n",
        "tree_reg.fit(psg_prepared, psg_labels)\n",
        "\n",
        "# we use 10 folds, which splits the data into 10 random folds\n",
        "# and evaluates the decision tree model 10 times\n",
        "# we test against a negative score cross-validation features expect\n",
        "# a utility function (greater is better) rather than a cost function\n",
        "# (where lower is better), this is the opposite of MSE\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(tree_reg, psg_prepared, psg_labels,\n",
        "                         scoring='neg_mean_squared_error', cv=10)\n",
        "tree_rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "# LEt's look at the results\n",
        "def display_scores(scores):\n",
        "  print('Scores:', scores)\n",
        "  print('Mean:', scores.mean())\n",
        "  print('Standard Deviation:', scores.std())\n",
        "\n",
        "display_scores(tree_rmse_scores)  \n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores: [0.40089186 0.68138514 0.26726124 0.53452248 0.18898224 0.23145502\n",
            " 0.         0.50452498 0.38138504 0.42640143]\n",
            "Mean: 0.36168094414418794\n",
            "Standard Deviation: 0.18577289890520673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi3Ex_WjyDUc",
        "colab_type": "code",
        "outputId": "6c402bb6-4982-4edc-93c9-204dd23c67db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "# let's compute the scores for the linear regression to be sure\n",
        "lin_scores = cross_val_score(lin_reg, psg_prepared, psg_labels,\n",
        "                             scoring = 'neg_mean_squared_error', cv=10)\n",
        "lin_rmse_scores = np.sqrt(-lin_scores)\n",
        "display_scores(lin_rmse_scores)\n",
        "# Woah, this is terrible after adding in the team dummy variables\n",
        "# will need unique data set if using LR"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores: [1.48470653e+09 1.33605265e+12 9.41821843e+09 3.22402470e+09\n",
            " 5.76610063e+08 9.45339866e+08 6.53911348e+09 1.21631926e+08\n",
            " 7.12768660e+08 1.83143566e+10]\n",
            "Mean: 137738941579.53882\n",
            "Standard Deviation: 399474371830.44824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlqMyX0iyNG1",
        "colab_type": "code",
        "outputId": "802eac73-903d-4ef2-ae6d-0e3c24ad323f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Let's try a random forest model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "forest_reg = RandomForestRegressor()\n",
        "forest_reg.fit(psg_prepared, psg_labels)\n",
        "psg_predictions = forest_reg.predict(psg_prepared)\n",
        "forest_mse = mean_squared_error(psg_labels, psg_predictions)\n",
        "forest_rmse = np.sqrt(forest_mse)\n",
        "forest_rmse\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10362634852063411"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du9RJAEhlugx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "f0fc3edb-58d0-49c4-e739-70886eb66684"
      },
      "source": [
        "scores = cross_val_score(forest_reg, psg_prepared, psg_labels,\n",
        "                         scoring='neg_mean_squared_error', cv=10)\n",
        "forest_rmse_scores = np.sqrt(-scores)\n",
        "display_scores(forest_rmse_scores)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores: [0.37824155 0.29303963 0.34532392 0.33230257 0.22810207 0.21525824\n",
            " 0.22016723 0.33140333 0.39579302 0.3453188 ]\n",
            "Mean: 0.3084950361641031\n",
            "Standard Deviation: 0.06283809076630241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv994c0JzRFd",
        "colab_type": "code",
        "outputId": "1beac5e5-90cb-4708-fad0-79cbd7b1e1ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "param_distribs = {\n",
        "        'n_estimators': randint(low=75, high=100),\n",
        "        'max_features': randint(low=75, high=110),\n",
        "    }\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
        "                                n_iter=30, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
        "rnd_search.fit(psg_prepared, psg_labels)\n",
        "# grid search can take a while, this is a randomized approach"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, error_score=nan,\n",
              "                   estimator=RandomForestRegressor(bootstrap=True,\n",
              "                                                   ccp_alpha=0.0,\n",
              "                                                   criterion='mse',\n",
              "                                                   max_depth=None,\n",
              "                                                   max_features='auto',\n",
              "                                                   max_leaf_nodes=None,\n",
              "                                                   max_samples=None,\n",
              "                                                   min_impurity_decrease=0.0,\n",
              "                                                   min_impurity_split=None,\n",
              "                                                   min_samples_leaf=1,\n",
              "                                                   min_samples_split=2,\n",
              "                                                   min_weight_fraction_leaf=0.0,\n",
              "                                                   n_estimators=100,\n",
              "                                                   n_jobs=None, oob_score=Fals...\n",
              "                                                   warm_start=False),\n",
              "                   iid='deprecated', n_iter=30, n_jobs=None,\n",
              "                   param_distributions={'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f4f8246ea20>,\n",
              "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f4f8246ebe0>},\n",
              "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
              "                   return_train_score=False, scoring='neg_mean_squared_error',\n",
              "                   verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkG5Z7Okzron",
        "colab_type": "code",
        "outputId": "5b0d1f32-ae84-44cb-f33a-cddc98aa3bc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "cvres = rnd_search.cv_results_\n",
        "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "    print(np.sqrt(-mean_score), params)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3181870694835285 {'max_features': 103, 'n_estimators': 89}\n",
            "0.31112709060418176 {'max_features': 82, 'n_estimators': 95}\n",
            "0.30766173703254424 {'max_features': 93, 'n_estimators': 97}\n",
            "0.3120119954556759 {'max_features': 85, 'n_estimators': 85}\n",
            "0.3093801848330712 {'max_features': 98, 'n_estimators': 95}\n",
            "0.3054934629899005 {'max_features': 98, 'n_estimators': 77}\n",
            "0.31643064416178257 {'max_features': 96, 'n_estimators': 95}\n",
            "0.3116414438599331 {'max_features': 76, 'n_estimators': 98}\n",
            "0.3119417994392123 {'max_features': 104, 'n_estimators': 80}\n",
            "0.31197990790328317 {'max_features': 76, 'n_estimators': 95}\n",
            "0.311537407109181 {'max_features': 107, 'n_estimators': 86}\n",
            "0.3184091836182232 {'max_features': 96, 'n_estimators': 86}\n",
            "0.314695313345991 {'max_features': 99, 'n_estimators': 91}\n",
            "0.31784096488946645 {'max_features': 101, 'n_estimators': 84}\n",
            "0.31192280047031007 {'max_features': 102, 'n_estimators': 90}\n",
            "0.31045599058466167 {'max_features': 89, 'n_estimators': 89}\n",
            "0.31027391598635884 {'max_features': 77, 'n_estimators': 79}\n",
            "0.3074781706751187 {'max_features': 81, 'n_estimators': 95}\n",
            "0.3217267067445901 {'max_features': 83, 'n_estimators': 81}\n",
            "0.31521636915545603 {'max_features': 92, 'n_estimators': 78}\n",
            "0.31463701852915726 {'max_features': 99, 'n_estimators': 88}\n",
            "0.32176759466240257 {'max_features': 83, 'n_estimators': 95}\n",
            "0.31154796947947916 {'max_features': 76, 'n_estimators': 94}\n",
            "0.31251356080205167 {'max_features': 102, 'n_estimators': 89}\n",
            "0.30670752878594 {'max_features': 81, 'n_estimators': 86}\n",
            "0.3112259610923783 {'max_features': 82, 'n_estimators': 89}\n",
            "0.31291605257863325 {'max_features': 109, 'n_estimators': 88}\n",
            "0.30683350027278145 {'max_features': 91, 'n_estimators': 78}\n",
            "0.31339810131405543 {'max_features': 78, 'n_estimators': 76}\n",
            "0.31275972699482474 {'max_features': 80, 'n_estimators': 96}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JQTwzbT1VB_",
        "colab_type": "code",
        "outputId": "58e5a9c5-89f9-4e9f-da3f-015705abdc3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# LEt's go back to the default\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "forest_reg = RandomForestRegressor()\n",
        "forest_reg.fit(psg_prepared, psg_labels)\n",
        "psg_predictions = forest_reg.predict(psg_prepared)\n",
        "forest_mse = mean_squared_error(psg_labels, psg_predictions)\n",
        "forest_rmse = np.sqrt(forest_mse)\n",
        "forest_rmse"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10716604006692872"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGbzMCdhQJVj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae350d6a-2df2-43c1-9509-3a28265f787a"
      },
      "source": [
        "# let's look at the relative importance of each attribute\n",
        "feature_importances = forest_reg.feature_importances_\n",
        "feature_importances\n",
        "attributes = list(psg_prepared)\n",
        "sorted(zip(feature_importances, attributes), reverse=True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.394721063744923, 'GC'),\n",
              " (0.09928634297230184, 'Classic'),\n",
              " (0.08535534584023337, 'Climber'),\n",
              " (0.08119845692239203, 'Tour'),\n",
              " (0.0745474598167044, 'Total'),\n",
              " (0.07222852576498669, 'TT'),\n",
              " (0.062015779870978234, 'Vuelta'),\n",
              " (0.022872706511235276, 'Sprint'),\n",
              " (0.02144011299765992, 'Team_UAE-Team Emirates'),\n",
              " (0.020406065532736415, 'Age'),\n",
              " (0.012899493115275247, 'lbl_win'),\n",
              " (0.009987618535962061, 'Team_0'),\n",
              " (0.007555431904770145, 'Giro'),\n",
              " (0.006271212303353144, 'Team_Astana Pro Team'),\n",
              " (0.006070807374029521, 'Team_Movistar Team'),\n",
              " (0.004815677535802662, 'amstel_win'),\n",
              " (0.004068462702917873, 'Team_Trek - Segafredo'),\n",
              " (0.003478053327195898, 'lombardia_win'),\n",
              " (0.001823425480765752, 'Team_Team Arkéa Samsic'),\n",
              " (0.001806975108637204, 'fleche_win'),\n",
              " (0.001695174203125459, 'Team_Mitchelton-Scott'),\n",
              " (0.0014083849978515737, 'san_remo_win'),\n",
              " (0.0010645435498366506, 'Team_NTT Pro Cycling'),\n",
              " (0.0006783822049476495, 'Team_Team Jumbo-Visma'),\n",
              " (0.0006616440218301409, 'e3_binckbank_win'),\n",
              " (0.0006405072140714911, 'Team_Matrix Powertag'),\n",
              " (0.00036935621897632415, 'Team_Team INEOS'),\n",
              " (0.0003533760473139477, 'Team_Israel Start-Up Nation'),\n",
              " (0.0001517246243916202, 'gent_win'),\n",
              " (0.00012788955479452055, 'Team_Team Sunweb'),\n",
              " (0.0, 'strade_win'),\n",
              " (0.0, 'roubaix_win'),\n",
              " (0.0, 'flanders_win'),\n",
              " (0.0, 'Team_À BLOC CT'),\n",
              " (0.0, 'Team_Work Service Dynatek Vega'),\n",
              " (0.0, 'Team_Wibatech Merx 7R'),\n",
              " (0.0, 'Team_WSA KTM Graz'),\n",
              " (0.0, 'Team_W52 / FC Porto'),\n",
              " (0.0, 'Team_Voster ATS Team'),\n",
              " (0.0, 'Team_VolkerWessels-Merckx'),\n",
              " (0.0, 'Team_Vino - Astana Motors'),\n",
              " (0.0, 'Team_Vini Zabù - KTM'),\n",
              " (0.0, 'Team_Utsunomiya Blitzen'),\n",
              " (0.0, 'Team_Uno-X Norwegian Development Team'),\n",
              " (0.0, 'Team_Tianyoude Hotel Cycling Team'),\n",
              " (0.0, 'Team_Terengganu Inc. TSG Cycling Team'),\n",
              " (0.0, 'Team_Team Vorarlberg Santic'),\n",
              " (0.0, 'Team_Team UKYO'),\n",
              " (0.0, 'Team_Team Total Direct Energie'),\n",
              " (0.0, 'Team_Team Skyline'),\n",
              " (0.0, 'Team_Team Sapura Cycling'),\n",
              " (0.0, 'Team_Team Novo Nordisk'),\n",
              " (0.0, 'Team_Team Medellin'),\n",
              " (0.0, 'Team_Team Hrinkow Advarics Cycleang'),\n",
              " (0.0, 'Team_Team Felbermayr - Simplon Wels'),\n",
              " (0.0, 'Team_Team Bridgestone Cycling'),\n",
              " (0.0, 'Team_Team BridgeLane'),\n",
              " (0.0, 'Team_Tarteletto - Isorex'),\n",
              " (0.0, 'Team_Swiss Racing Academy'),\n",
              " (0.0, 'Team_St Michel - Auber93'),\n",
              " (0.0, 'Team_St George Continental Cycling Team'),\n",
              " (0.0, 'Team_Sport Vlaanderen - Baloise'),\n",
              " (0.0, 'Team_Shenzhen Xidesheng Cycling Team'),\n",
              " (0.0, 'Team_Salcano Sakarya BB Team'),\n",
              " (0.0, 'Team_SSOIS Miogee Cycling Team'),\n",
              " (0.0, 'Team_SEG Racing Academy'),\n",
              " (0.0, 'Team_Riwal Readynez Cycling Team'),\n",
              " (0.0, 'Team_Rally Cycling'),\n",
              " (0.0, 'Team_Radio Popular Boavista'),\n",
              " (0.0, 'Team_ProTouch'),\n",
              " (0.0, 'Team_Pgn Road Cycling Team'),\n",
              " (0.0, \"Team_Oliver's Real Food\"),\n",
              " (0.0, 'Team_Ningxia Sports Lottery Continental Team'),\n",
              " (0.0, 'Team_Natura4Ever - Roubaix Lille Métropole'),\n",
              " (0.0, 'Team_NTT Continental Cycling Team'),\n",
              " (0.0, 'Team_NIPPO DELKO One Provence'),\n",
              " (0.0, 'Team_Miranda - Mortágua'),\n",
              " (0.0, 'Team_Minsk Cycling Club'),\n",
              " (0.0, 'Team_Metec - TKH Continental Cyclingteam p/b Mantel'),\n",
              " (0.0, 'Team_Memil - CCN Pro Cycling'),\n",
              " (0.0, 'Team_Mazowsze Serce Polski'),\n",
              " (0.0, 'Team_Lotto Soudal'),\n",
              " (0.0, 'Team_Lokosphinx'),\n",
              " (0.0, 'Team_Leopard Pro Cycling'),\n",
              " (0.0, 'Team_Kometa Xstra Cycling Team'),\n",
              " (0.0, 'Team_Kinan Cycling Team'),\n",
              " (0.0, 'Team_Kelly / InOutBuild / UD Oliveirense'),\n",
              " (0.0, 'Team_Israel Cycling Academy'),\n",
              " (0.0, 'Team_HKSI Pro Cycling Team'),\n",
              " (0.0, 'Team_Groupement Sportif des Pétroliers'),\n",
              " (0.0, 'Team_Groupama - FDJ'),\n",
              " (0.0, 'Team_Giotti Victoria'),\n",
              " (0.0, 'Team_Gazprom - RusVelo'),\n",
              " (0.0, 'Team_Fundación - Orbea'),\n",
              " (0.0, 'Team_Ferei Pro Cycling Team'),\n",
              " (0.0, 'Team_Equipo Kern Pharma'),\n",
              " (0.0, 'Team_Equipo Continental Supergiros'),\n",
              " (0.0, 'Team_Equipe continentale Groupama-FDJ'),\n",
              " (0.0, 'Team_Elkov - Kasper'),\n",
              " (0.0, 'Team_Efapel'),\n",
              " (0.0, 'Team_EF Pro Cycling'),\n",
              " (0.0, 'Team_Deceuninck - Quick Step'),\n",
              " (0.0, 'Team_Cycling Team Friuli ASD'),\n",
              " (0.0, 'Team_Colombia Tierra de Atletas - GW Bicicletas'),\n",
              " (0.0, 'Team_Cofidis, Solutions Crédits'),\n",
              " (0.0, 'Team_Circus - Wanty Gobert'),\n",
              " (0.0, 'Team_Canyon dhb p/b Soreen'),\n",
              " (0.0, \"Team_Canel's Pro Cycling\"),\n",
              " (0.0, 'Team_Cambodia Cycling Academy'),\n",
              " (0.0, 'Team_Caja Rural - Seguros RGA'),\n",
              " (0.0, 'Team_CCC Team'),\n",
              " (0.0, 'Team_CCC Development Team'),\n",
              " (0.0, 'Team_Burgos-BH'),\n",
              " (0.0, 'Team_Black Spoke Pro Cycling Academy'),\n",
              " (0.0, 'Team_Bingoal - Wallonie Bruxelles'),\n",
              " (0.0, 'Team_Bike Aid'),\n",
              " (0.0, 'Team_Benediction Ignite XL'),\n",
              " (0.0, 'Team_Bardiani-CSF-Faizanè'),\n",
              " (0.0, 'Team_Bahrain Cycling Academy'),\n",
              " (0.0, 'Team_Bahrain - McLaren'),\n",
              " (0.0, 'Team_BORA - hansgrohe'),\n",
              " (0.0, 'Team_BHS - PL Beton Bornholm'),\n",
              " (0.0, 'Team_BEAT Cycling Club'),\n",
              " (0.0, 'Team_B&B Hotels - Vital Concept p/b KTM'),\n",
              " (0.0, 'Team_Aviludo-Louletano'),\n",
              " (0.0, 'Team_Atum general / Tavira / Maria Nova Hotel'),\n",
              " (0.0, 'Team_Androni Giocattoli - Sidermec'),\n",
              " (0.0, 'Team_Amore & Vita - Prodir'),\n",
              " (0.0, 'Team_Alpecin-Fenix'),\n",
              " (0.0, 'Team_Aisan Racing Team'),\n",
              " (0.0, 'Team_Adria Mobil'),\n",
              " (0.0, 'Team_AG2R La Mondiale'),\n",
              " (0.0, 'Team_7 Eleven Cliqq - air21 by Roadbike Philippines')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xNpdblU6GeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's evaluate the model on the test set\n",
        "final_model = forest_reg\n",
        "x_test = test_set.drop('Paris_Nice_etc', axis=1)\n",
        "y_test = test_set['Paris_Nice_etc'].copy()\n",
        "test_rider = test_set['Rider'].copy()\n",
        "\n",
        "########\n",
        "# Adjust the data\n",
        "# let's normalize the data\n",
        "x_test['GC'] = preprocessing.scale(x_test['GC'])\n",
        "x_test['Sprint'] = preprocessing.scale(x_test['Sprint'])\n",
        "x_test['TT'] = preprocessing.scale(x_test['TT'])\n",
        "x_test['Climber'] = preprocessing.scale(x_test['Climber'])\n",
        "x_test['Classic'] = preprocessing.scale(x_test['Classic'])\n",
        "x_test['Age'] = preprocessing.scale(x_test['Age'])\n",
        "x_test['Giro'] = preprocessing.scale(x_test['Giro'])\n",
        "x_test['Vuelta'] = preprocessing.scale(x_test['Vuelta'])\n",
        "x_test['Tour'] = preprocessing.scale(x_test['Tour'])\n",
        "x_test['Total'] = preprocessing.scale(x_test['Total'])\n",
        "x_test['san_remo_win'] = preprocessing.scale(x_test['san_remo_win'])\n",
        "x_test['lombardia_win'] = preprocessing.scale(x_test['lombardia_win'])\n",
        "x_test['flanders_win'] = preprocessing.scale(x_test['flanders_win'])\n",
        "x_test['lbl_win'] = preprocessing.scale(x_test['lbl_win'])\n",
        "x_test['roubaix_win'] = preprocessing.scale(x_test['roubaix_win'])\n",
        "x_test['strade_win'] = preprocessing.scale(x_test['strade_win'])\n",
        "#x_test['suisse_win'] = preprocessing.scale(x_test['suisse_win'])\n",
        "x_test['fleche_win'] = preprocessing.scale(x_test['fleche_win'])\n",
        "x_test['gent_win'] = preprocessing.scale(x_test['gent_win'])\n",
        "#x_test['Paris_Nice_etc'] = preprocessing.scale(x_test['Paris_Nice_etc'])\n",
        "x_test['e3_binckbank_win'] = preprocessing.scale(x_test['e3_binckbank_win'])\n",
        "x_test['amstel_win'] = preprocessing.scale(x_test['amstel_win'])\n",
        "#x_test['dauphine_win'] = preprocessing.scale(x_test['dauphine_win'])\n",
        "#x_test['romandie_win'] = preprocessing.scale(x_test['romandie_win'])\n",
        "\n",
        "# Algorithims prefer working with numbers generally, so let's convert the team\n",
        "# proximity columns to numeric values\n",
        "x_test['Team'] = x_test['Team'].astype(str)\n",
        "\n",
        "# use pd.concat to join the new columns with your original dataframe\n",
        "x_test = pd.concat([x_test,pd.get_dummies(x_test['Team'], prefix='Team')],axis=1)\n",
        "\n",
        "# now drop the original 'country' column (you don't need it anymore)\n",
        "x_test.drop(['Team'],axis=1, inplace=True)\n",
        "\n",
        "# Drop rider name\n",
        "x_test = x_test.drop(['Rider'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOLODNkgnQFz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "e8fa9414-7d33-4f7e-ecee-e0154e41d08f"
      },
      "source": [
        "########\n",
        "\n",
        "x_test_prepared = x_test\n",
        "final_predictions = final_model.predict(x_test_prepared)\n",
        "\n",
        "final_mse = mean_squared_error(y_test, final_predictions)\n",
        "final_rmse = np.sqrt(final_mse)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-4b3d5ae82ca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_test_prepared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfinal_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_prepared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfinal_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    389\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 124 and input n_features is 89 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpSxYQwrnXjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's look at top riders"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}