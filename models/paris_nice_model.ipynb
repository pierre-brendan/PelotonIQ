{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "paris_nice_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkZojNsoEVeNY0Kzfn9LAh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pierre-brendan/PelotonIQ/blob/master/models/paris_nice_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBT1R8ObJoSU",
        "colab_type": "text"
      },
      "source": [
        "# First attempt at a model to predict the winner of Paris - Nice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSXKvTvZJjO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount drive to get cyclist data\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tZ6OqLLJysB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All this code is from the Kmeans clustering code\n",
        "\n",
        "# load modules\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# load the rider data\n",
        "cycling_data = pd.read_csv('/content/drive/My Drive/cycling_data/rider_data.csv')\n",
        "cycling_data = cycling_data[cycling_data['Rider'] != 'Bjorg Lambrecht ']\n",
        "cycling_data = cycling_data[cycling_data['Rider'] != 'Robbert de Greef ']\n",
        "\n",
        "# Load the GC data\n",
        "gc_winners = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/gc_winners_consolidated.csv')\n",
        "gc_winners.rename(columns = {'Cyclist':'Rider'}, inplace = True)\n",
        "\n",
        "# Merge the GC winners to the rider data set\n",
        "cycling_data = pd.merge(cycling_data, gc_winners, how='left', on='Rider')\n",
        "\n",
        "# Load in the Monuments data\n",
        "san_remo = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/san_remo.csv')\n",
        "san_remo = san_remo['san_remo_win'].groupby(san_remo['Rider']).sum()\n",
        "roubaix = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/roubaix.csv')\n",
        "roubaix = roubaix['roubaix_win'].groupby(roubaix['Rider']).sum()\n",
        "flanders = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/flanders.csv')\n",
        "flanders = flanders['flanders_win'].groupby(flanders['Rider']).sum()\n",
        "lombardia = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/lombardia.csv')\n",
        "lombardia = lombardia['lombardia_win'].groupby(lombardia['Rider']).sum()\n",
        "liege = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/liege.csv')\n",
        "liege = liege['lbl_win'].groupby(liege['Rider']).sum()\n",
        "strade = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/strade.csv')\n",
        "strade['strade_win'] = 1\n",
        "strade = strade['strade_win'].groupby(strade['Rider']).sum()\n",
        "suisse = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/suisse.csv')\n",
        "suisse['suisse_win'] = 1\n",
        "suisse = suisse['suisse_win'].groupby(suisse['Rider']).sum()\n",
        "romandie = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/romandie.csv')\n",
        "romandie['romandie_win'] = 1\n",
        "romandie = romandie['romandie_win'].groupby(romandie['Rider']).sum()\n",
        "paris_nice = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/paris_nice.csv')\n",
        "paris_nice['paris_nice_win'] = 1\n",
        "paris_nice = paris_nice['paris_nice_win'].groupby(paris_nice['Rider']).sum()\n",
        "gent = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/gent.csv')\n",
        "gent['gent_win'] = 1\n",
        "gent = gent['gent_win'].groupby(gent['Rider']).sum()\n",
        "fleche = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/fleche.csv')\n",
        "fleche['fleche_win'] = 1\n",
        "fleche = fleche['fleche_win'].groupby(fleche['Rider']).sum()\n",
        "e3_binckbank = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/e3_binckbank.csv')\n",
        "e3_binckbank['e3_binckbank_win'] = 1\n",
        "e3_binckbank = e3_binckbank['e3_binckbank_win'].groupby(e3_binckbank['Rider']).sum()\n",
        "dauphine = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/dauphine.csv')\n",
        "dauphine['dauphine_win'] = 1\n",
        "dauphine = dauphine['dauphine_win'].groupby(dauphine['Rider']).sum()\n",
        "amstel = pd.read_csv('/content/drive/My Drive/cycling_data/Historical_GC_Classics_Results/amstel.csv')\n",
        "amstel['amstel_win'] = 1\n",
        "amstel = amstel['amstel_win'].groupby(amstel['Rider']).sum()\n",
        "\n",
        "\n",
        "# Merge the Monuments winners to the rider data set\n",
        "cycling_data = pd.merge(cycling_data, san_remo, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, flanders, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, liege, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, lombardia, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, roubaix, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, strade, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, suisse, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, fleche, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, gent, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, paris_nice, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, e3_binckbank, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, amstel, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, dauphine, how='left', on='Rider')\n",
        "cycling_data = pd.merge(cycling_data, romandie, how='left', on='Rider')\n",
        "\n",
        "# Replace NaN with 0's\n",
        "cycling_data.fillna(0, inplace=True)\n",
        "\n",
        "# Drop country variable\n",
        "cycling_data = cycling_data.drop(['Country', 'crawl_date'], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNIWRKzCKPUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cycling_data.describe()\n",
        "# Okay, let's make a column for winners of Paris-Nice type races\n",
        "# Currently I'm thinking Paris - Nice, Tour de Suisse and Dauphine (Tierrento is not in the data set currently)\n",
        "cycling_data['Paris_Nice_etc'] = cycling_data['paris_nice_win'] + cycling_data['dauphine_win'] + cycling_data['suisse_win']\n",
        "\n",
        "# Will need to drop those 3 columns now\n",
        "cycling_data = cycling_data.drop(['paris_nice_win', 'dauphine_win', 'suisse_win'], axis = 1)\n",
        "\n",
        "# Split the data sets\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_set, test_set = train_test_split(cycling_data, test_size = 0.1, \n",
        "                                       random_state = 42)\n",
        "# first let's make a fresh copy of our data and drop what we are trying\n",
        "# to predict so it doesn't have any transmations applied to it\n",
        "psg = train_set.drop('Paris_Nice_etc', axis=1)\n",
        "psg_labels = train_set['Paris_Nice_etc'].copy()\n",
        "psg_rider = train_set['Rider'].copy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOcvD8lLKL4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Also from KMeans code\n",
        "# This would be the best approach to removing special characters\n",
        "#dictionary = {'ž':'z', 'č':'c', 'ü':'u', 'á':'a', 'é':'e', 'ö':'o', 'ó':'o', 'ç':'c',\n",
        "#              'ń':'ń', 'í':'i', 'è':'e'}\n",
        "#cycling_data.replace(dictionary, regex=True, inplace=True)\n",
        "\n",
        "#d = psg.drop(['Rider'], axis=1) #, 'Team'\n",
        "\n",
        "# let's normalize the data\n",
        "psg['GC'] = preprocessing.scale(psg['GC'])\n",
        "psg['Sprint'] = preprocessing.scale(psg['Sprint'])\n",
        "psg['TT'] = preprocessing.scale(psg['TT'])\n",
        "psg['Climber'] = preprocessing.scale(psg['Climber'])\n",
        "psg['Classic'] = preprocessing.scale(psg['Classic'])\n",
        "psg['Age'] = preprocessing.scale(psg['Age'])\n",
        "psg['Giro'] = preprocessing.scale(psg['Giro'])\n",
        "psg['Vuelta'] = preprocessing.scale(psg['Vuelta'])\n",
        "psg['Tour'] = preprocessing.scale(psg['Tour'])\n",
        "psg['Total'] = preprocessing.scale(psg['Total'])\n",
        "psg['san_remo_win'] = preprocessing.scale(psg['san_remo_win'])\n",
        "psg['lombardia_win'] = preprocessing.scale(psg['lombardia_win'])\n",
        "psg['flanders_win'] = preprocessing.scale(psg['flanders_win'])\n",
        "psg['lbl_win'] = preprocessing.scale(psg['lbl_win'])\n",
        "psg['roubaix_win'] = preprocessing.scale(psg['roubaix_win'])\n",
        "psg['strade_win'] = preprocessing.scale(psg['strade_win'])\n",
        "#psg['suisse_win'] = preprocessing.scale(psg['suisse_win'])\n",
        "psg['fleche_win'] = preprocessing.scale(psg['fleche_win'])\n",
        "psg['gent_win'] = preprocessing.scale(psg['gent_win'])\n",
        "#psg['Paris_Nice_etc'] = preprocessing.scale(psg['Paris_Nice_etc'])\n",
        "psg['e3_binckbank_win'] = preprocessing.scale(psg['e3_binckbank_win'])\n",
        "psg['amstel_win'] = preprocessing.scale(psg['amstel_win'])\n",
        "#psg['dauphine_win'] = preprocessing.scale(psg['dauphine_win'])\n",
        "psg['romandie_win'] = preprocessing.scale(psg['romandie_win'])\n",
        "\n",
        "# Algorithims prefer working with numbers generally, so let's convert the team\n",
        "# proximity columns to numeric values\n",
        "psg['Team'] = psg['Team'].astype(str)\n",
        "housing_cat = psg[['Team']]\n",
        "# Problem is if we use numeric examples for these variables algorithims\n",
        "# will think they are near eachother. Instead we should make some one-hot\n",
        "# encodings otherwise known as dummy variables for each category as it's own\n",
        "# column\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "cat_encoder = OrdinalEncoder()\n",
        "psg_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "cat_attribs = ['Team']\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "      ('cat', OneHotEncoder(), cat_attribs),\n",
        "])\n",
        "\n",
        "psg_prepared = full_pipeline.fit_transform(psg) # applying this to pandas dataframe called housing\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwOaWj0tP9qV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pd.DataFrame(psg_prepared)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3qspSfrMwIY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "db6f1370-272e-4b66-f14a-96ebe394036c"
      },
      "source": [
        "# Load this for RMSE calcs\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Linear regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(psg_prepared, psg_labels)\n",
        "\n",
        "# Let's test more powerful model\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "# Build decision tree model\n",
        "tree_reg = DecisionTreeRegressor()\n",
        "tree_reg.fit(psg_prepared, psg_labels)\n",
        "\n",
        "# we use 10 folds, which splits the data into 10 random folds\n",
        "# and evaluates the decision tree model 10 times\n",
        "# we test against a negative score cross-validation features expect\n",
        "# a utility function (greater is better) rather than a cost function\n",
        "# (where lower is better), this is the opposite of MSE\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(tree_reg, psg_prepared, psg_labels,\n",
        "                         scoring='neg_mean_squared_error', cv=10)\n",
        "tree_rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "# LEt's look at the results\n",
        "def display_scores(scores):\n",
        "  print('Scores:', scores)\n",
        "  print('Mean:', scores.mean())\n",
        "  print('Standard Deviation:', scores.std())\n",
        "\n",
        "display_scores(tree_rmse_scores)  \n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores: [0.23301831 0.10737354 0.07325499 0.281018   0.11843882 0.20232047\n",
            " 0.23593131 0.10618884 0.41895675 0.48534483]\n",
            "Mean: 0.22618458562992302\n",
            "Standard Deviation: 0.13097888877940567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi3Ex_WjyDUc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "646dca62-e09f-4f02-f343-992bdd7f4fee"
      },
      "source": [
        "# let's compute the scores for the linear regression to be sure\n",
        "lin_scores = cross_val_score(lin_reg, psg_prepared, psg_labels,\n",
        "                             scoring = 'neg_mean_squared_error', cv=10)\n",
        "lin_rmse_scores = np.sqrt(-lin_scores)\n",
        "display_scores(lin_rmse_scores)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores: [0.23303312 0.1074048  0.07332832 0.28102334 0.11848032 0.20233712\n",
            " 0.23595805 0.10622028 0.41896886 0.48534852]\n",
            "Mean: 0.2262102723084582\n",
            "Standard Deviation: 0.13096390595711432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlqMyX0iyNG1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "c2e7dc1a-e8a9-4da2-f2b8-c186b1c176a0"
      },
      "source": [
        "# Let's try a random forest model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "forest_reg = RandomForestRegressor()\n",
        "forest_reg.fit(psg_prepared, psg_labels)\n",
        "psg_predictions = forest_reg.predict(psg_prepared)\n",
        "forest_mse = mean_squared_error(psg_labels, psg_predictions)\n",
        "forest_rmse = np.sqrt(forest_mse)\n",
        "forest_rmse\n",
        "scores = cross_val_score(forest_reg, psg_prepared, psg_labels,\n",
        "                         scoring='neg_mean_squared_error', cv=10)\n",
        "forest_rmse_scores = np.sqrt(-scores)\n",
        "display_scores(forest_rmse_scores)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores: [0.23194342 0.1048063  0.07329133 0.28073032 0.11387256 0.20142986\n",
            " 0.24074062 0.11194304 0.41794336 0.4847014 ]\n",
            "Mean: 0.2261402201401077\n",
            "Standard Deviation: 0.13084708010612128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv994c0JzRFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "3be501a5-631e-4633-bdb3-c5fa8380105b"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "param_distribs = {\n",
        "        'n_estimators': randint(low=500, high=700),\n",
        "        'max_features': randint(low=100, high=110),\n",
        "    }\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
        "                                n_iter=20, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
        "rnd_search.fit(psg_prepared, psg_labels)\n",
        "# grid search can take a while, this is a randomized approach"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, error_score=nan,\n",
              "                   estimator=RandomForestRegressor(bootstrap=True,\n",
              "                                                   ccp_alpha=0.0,\n",
              "                                                   criterion='mse',\n",
              "                                                   max_depth=None,\n",
              "                                                   max_features='auto',\n",
              "                                                   max_leaf_nodes=None,\n",
              "                                                   max_samples=None,\n",
              "                                                   min_impurity_decrease=0.0,\n",
              "                                                   min_impurity_split=None,\n",
              "                                                   min_samples_leaf=1,\n",
              "                                                   min_samples_split=2,\n",
              "                                                   min_weight_fraction_leaf=0.0,\n",
              "                                                   n_estimators=100,\n",
              "                                                   n_jobs=None, oob_score=Fals...\n",
              "                                                   warm_start=False),\n",
              "                   iid='deprecated', n_iter=20, n_jobs=None,\n",
              "                   param_distributions={'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fa057c2d908>,\n",
              "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fa057c2d780>},\n",
              "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
              "                   return_train_score=False, scoring='neg_mean_squared_error',\n",
              "                   verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkG5Z7Okzron",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "336954ea-51f0-47aa-87ff-f6a172ec0ff2"
      },
      "source": [
        "cvres = rnd_search.cv_results_\n",
        "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "    print(np.sqrt(-mean_score), params)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2623612783599989 {'max_features': 106, 'n_estimators': 535}\n",
            "0.26293954272548936 {'max_features': 107, 'n_estimators': 120}\n",
            "0.2625863107516486 {'max_features': 106, 'n_estimators': 221}\n",
            "0.26261211179396593 {'max_features': 102, 'n_estimators': 314}\n",
            "0.2623766392704155 {'max_features': 107, 'n_estimators': 472}\n",
            "0.26273274698903426 {'max_features': 103, 'n_estimators': 230}\n",
            "0.26251459330734084 {'max_features': 105, 'n_estimators': 408}\n",
            "0.26238919093737856 {'max_features': 101, 'n_estimators': 443}\n",
            "0.26233600053959666 {'max_features': 105, 'n_estimators': 485}\n",
            "0.26259560199925686 {'max_features': 104, 'n_estimators': 260}\n",
            "0.2629272753014414 {'max_features': 109, 'n_estimators': 121}\n",
            "0.2624069620663564 {'max_features': 108, 'n_estimators': 660}\n",
            "0.2622871483914505 {'max_features': 109, 'n_estimators': 575}\n",
            "0.26242299849659856 {'max_features': 102, 'n_estimators': 666}\n",
            "0.26227092671750507 {'max_features': 103, 'n_estimators': 604}\n",
            "0.26232152399022574 {'max_features': 102, 'n_estimators': 584}\n",
            "0.26293954272548936 {'max_features': 102, 'n_estimators': 120}\n",
            "0.26260993972984764 {'max_features': 108, 'n_estimators': 266}\n",
            "0.2623550095933668 {'max_features': 101, 'n_estimators': 487}\n",
            "0.26248229016648533 {'max_features': 108, 'n_estimators': 415}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JQTwzbT1VB_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b390fa56-1d77-4021-d719-eafdf58a3de1"
      },
      "source": [
        "psg_prepared"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<718x110 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 718 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}