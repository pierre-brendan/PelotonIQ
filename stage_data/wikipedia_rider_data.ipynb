{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wikipedia_rider_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPgc9gB1Wi80aKz9GRTARZ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pierre-brendan/PelotonIQ/blob/master/stage_data/wikipedia_rider_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3N4r_MOWK2N",
        "colab_type": "text"
      },
      "source": [
        "Wikipedia is a great source for some cycling data like Monuments and Grand Tour wins by year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9AEvixwQYDo",
        "colab_type": "text"
      },
      "source": [
        "# Monuments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK0SmPpbIfQc",
        "colab_type": "text"
      },
      "source": [
        "**San Remo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbuKiy4S-gmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import packages\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG9KNwojWGeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start with Milan-San Remo\n",
        "url = 'https://en.wikipedia.org/wiki/Milan%E2%80%93San_Remo'\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "# Milan San Remo\n",
        "tables = soup.find('table', class_='sortable')\n",
        "links = tables.find_all('a')\n",
        "\n",
        "Year = []\n",
        "for link in links:\n",
        "    Year.append(link.get('title'))\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['tmp'] = Year\n",
        "df = df[-200:] \n",
        "year = df.iloc[::4, :]\n",
        "year['c'] = range(len(year))\n",
        "country = df.iloc[1::4, :]\n",
        "country['c'] = range(len(country))\n",
        "rider = df.iloc[2::4, :]\n",
        "rider['c'] = range(len(rider))\n",
        "team = df.iloc[3::4, :]\n",
        "team['c'] = range(len(team))\n",
        "\n",
        "# join the columns\n",
        "san_remo = pd.merge(year, rider, how='left', on='c')\n",
        "san_remo = pd.merge(san_remo, team, how='left', on='c')\n",
        "\n",
        "\n",
        "# Rename the columns\n",
        "san_remo.columns = ['Year', 'c', 'Rider','Team']\n",
        "san_remo = pd.DataFrame(san_remo)\n",
        "\n",
        "# Drop the 'C' column\n",
        "san_remo.drop(['c'], axis=1, inplace=True)\n",
        "\n",
        "# remove (cyclist) from names\n",
        "san_remo['Rider'] = san_remo['Rider'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "san_remo['Team'] = san_remo['Team'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "\n",
        "# add a binary variable for the winners\n",
        "san_remo['san_remo_win'] = 1\n",
        "\n",
        "# save the file\n",
        "san_remo.to_csv('san_remo.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGsh_0uZIqCl",
        "colab_type": "text"
      },
      "source": [
        "**Flanders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZf9elP9XIzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flanders\n",
        "url = 'https://en.wikipedia.org/wiki/Tour_of_Flanders'\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "tables = soup.find('table', class_='sortable')\n",
        "links = tables.find_all('a')\n",
        "\n",
        "Year = []\n",
        "for link in links:\n",
        "    Year.append(link.get('title'))\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['tmp'] = Year\n",
        "df = df[-200:] \n",
        "year = df.iloc[::4, :]\n",
        "year['c'] = range(len(year))\n",
        "country = df.iloc[1::4, :]\n",
        "country['c'] = range(len(country))\n",
        "rider = df.iloc[2::4, :]\n",
        "rider['c'] = range(len(rider))\n",
        "team = df.iloc[3::4, :]\n",
        "team['c'] = range(len(team))\n",
        "\n",
        "# join the columns\n",
        "flanders = pd.merge(year, rider, how='left', on='c')\n",
        "flanders = pd.merge(flanders, team, how='left', on='c')\n",
        "\n",
        "# Rename the columns\n",
        "flanders.columns = ['Year', 'c', 'Rider','Team']\n",
        "flanders = pd.DataFrame(flanders)\n",
        "\n",
        "# Drop the 'C' column\n",
        "flanders.drop(['c'], axis=1, inplace=True)\n",
        "\n",
        "# remove (cyclist) from names\n",
        "flanders['Rider'] = flanders['Rider'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "flanders['Team'] = flanders['Team'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "\n",
        "# add a binary variable for the winners\n",
        "flanders['flanders_win'] = 1\n",
        "\n",
        "# save the file\n",
        "flanders.to_csv('flanders.csv', index=False, encoding='utf-8')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0pz6X71JhaM",
        "colab_type": "text"
      },
      "source": [
        "**Paris–Roubaix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0QBQhecJOg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# roubaix\n",
        "url = 'https://en.wikipedia.org/wiki/Paris%E2%80%93Roubaix'\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "\n",
        "tables = soup.find('table', class_='sortable')\n",
        "links = tables.find_all('a')\n",
        "\n",
        "Year = []\n",
        "for link in links:\n",
        "    Year.append(link.get('title'))\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['tmp'] = Year\n",
        "df = df[-140:] \n",
        "year = df.iloc[::4, :]\n",
        "year['c'] = range(len(year))\n",
        "country = df.iloc[1::4, :]\n",
        "country['c'] = range(len(country))\n",
        "rider = df.iloc[2::4, :]\n",
        "rider['c'] = range(len(rider))\n",
        "team = df.iloc[3::4, :]\n",
        "team['c'] = range(len(team))\n",
        "\n",
        "# join the columns\n",
        "roubaix = pd.merge(year, rider, how='left', on='c')\n",
        "roubaix = pd.merge(roubaix, team, how='left', on='c')\n",
        "\n",
        "\n",
        "# Rename the columns\n",
        "roubaix.columns = ['Year', 'c', 'Rider','Team']\n",
        "roubaix = pd.DataFrame(roubaix)\n",
        "\n",
        "# Drop the 'C' column\n",
        "roubaix.drop(['c'], axis=1, inplace=True)\n",
        "\n",
        "# remove (cyclist) from names\n",
        "roubaix['Rider'] = roubaix['Rider'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "roubaix['Team'] = roubaix['Team'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "\n",
        "# add a binary variable for the winners\n",
        "roubaix['roubaix_win'] = 1\n",
        "\n",
        "# save the file\n",
        "roubaix.to_csv('roubaix.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_xtdRA89-0H",
        "colab_type": "text"
      },
      "source": [
        "**Liege**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQTBV4luKE9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LBL\n",
        "url = 'https://en.wikipedia.org/wiki/Li%C3%A8ge%E2%80%93Bastogne%E2%80%93Li%C3%A8ge'\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "\n",
        "tables = soup.find('table', class_='sortable')\n",
        "links = tables.find_all('a')\n",
        "\n",
        "Year = []\n",
        "for link in links:\n",
        "    Year.append(link.get('title'))\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['tmp'] = Year\n",
        "df = df[-140:] \n",
        "year = df.iloc[::4, :]\n",
        "year['c'] = range(len(year))\n",
        "country = df.iloc[1::4, :]\n",
        "country['c'] = range(len(country))\n",
        "rider = df.iloc[2::4, :]\n",
        "rider['c'] = range(len(rider))\n",
        "team = df.iloc[3::4, :]\n",
        "team['c'] = range(len(team))\n",
        "\n",
        "# join the columns\n",
        "lbl = pd.merge(year, rider, how='left', on='c')\n",
        "lbl = pd.merge(lbl, team, how='left', on='c')\n",
        "\n",
        "\n",
        "# Rename the columns\n",
        "lbl.columns = ['Year', 'c', 'Rider','Team']\n",
        "lbl = pd.DataFrame(lbl)\n",
        "\n",
        "# Drop the 'C' column\n",
        "lbl.drop(['c'], axis=1, inplace=True)\n",
        "\n",
        "# remove (cyclist) from names\n",
        "lbl['Rider'] = lbl['Rider'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "lbl['Team'] = lbl['Team'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "\n",
        "# add a binary variable for the winners\n",
        "lbl['lbl_win'] = 1\n",
        "\n",
        "# save the file\n",
        "lbl.to_csv('liege.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIBOm-56_UeU",
        "colab_type": "text"
      },
      "source": [
        "**Lombardia**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLN6K2pi_Tvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lombardia\n",
        "url = 'https://en.wikipedia.org/wiki/Giro_di_Lombardia'\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "\n",
        "tables = soup.find('table', class_='sortable')\n",
        "links = tables.find_all('a')\n",
        "\n",
        "Year = []\n",
        "for link in links:\n",
        "    Year.append(link.get('title'))\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['tmp'] = Year\n",
        "df = df[-140:] \n",
        "year = df.iloc[::4, :]\n",
        "year['c'] = range(len(year))\n",
        "country = df.iloc[1::4, :]\n",
        "country['c'] = range(len(country))\n",
        "rider = df.iloc[2::4, :]\n",
        "rider['c'] = range(len(rider))\n",
        "team = df.iloc[3::4, :]\n",
        "team['c'] = range(len(team))\n",
        "\n",
        "# join the columns\n",
        "lombardia = pd.merge(year, rider, how='left', on='c')\n",
        "lombardia = pd.merge(lombardia, team, how='left', on='c')\n",
        "\n",
        "\n",
        "# Rename the columns\n",
        "lombardia.columns = ['Year', 'c', 'Rider','Team']\n",
        "lombardia = pd.DataFrame(lombardia)\n",
        "\n",
        "# Drop the 'C' column\n",
        "lombardia.drop(['c'], axis=1, inplace=True)\n",
        "\n",
        "# remove (cyclist) from names\n",
        "lombardia['Rider'] = lombardia['Rider'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "lombardia['Team'] = lombardia['Team'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "\n",
        "# add a binary variable for the winners\n",
        "lombardia['lombardia_win'] = 1\n",
        "\n",
        "# save the file\n",
        "lombardia.to_csv('lombardia.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPJi8ZejWp-M",
        "colab_type": "text"
      },
      "source": [
        "# GC winners from the Grand Tours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVne0lVpWvhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TDF\n",
        "url = 'https://en.wikipedia.org/wiki/List_of_Tour_de_France_general_classification_winners'\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "tables = soup.find('table', class_='sortable')\n",
        "links = tables.find_all('a')\n",
        "\n",
        "Year = []\n",
        "for link in links:\n",
        "    Year.append(link.get('title'))\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['tmp'] = Year\n",
        "df = df[-200:] \n",
        "year = df.iloc[::4, :]\n",
        "year['c'] = range(len(year))\n",
        "country = df.iloc[1::4, :]\n",
        "country['c'] = range(len(country))\n",
        "rider = df.iloc[2::4, :]\n",
        "rider['c'] = range(len(rider))\n",
        "team = df.iloc[3::4, :]\n",
        "team['c'] = range(len(team))\n",
        "\n",
        "# join the columns\n",
        "tdf = pd.merge(year, rider, how='left', on='c')\n",
        "tdf = pd.merge(tdf, team, how='left', on='c')\n",
        "\n",
        "# Rename the columns\n",
        "tdf.columns = ['Year', 'c', 'Rider','Team']\n",
        "tdf = pd.DataFrame(tdf)\n",
        "\n",
        "# Drop the 'C' column\n",
        "tdf.drop(['c'], axis=1, inplace=True)\n",
        "\n",
        "# remove (cyclist) from names\n",
        "tdf['Rider'] = tdf['Rider'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "tdf['Team'] = tdf['Team'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "\n",
        "# add a binary variable for the winners\n",
        "tdf['tdf_win'] = 1\n",
        "\n",
        "# Let's only do the last 9 years since it gets all messed up\n",
        "tdf = tdf.tail(9)\n",
        "\n",
        "# save the file\n",
        "#san_remo.to_csv('san_remo.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmZFHgr5aw7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Giro d'italia\n",
        "# giro\n",
        "url = 'https://en.wikipedia.org/wiki/List_of_Giro_d%27Italia_general_classification_winners'\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "tables = soup.find('table', class_='sortable')\n",
        "links = tables.find_all('a')\n",
        "\n",
        "Year = []\n",
        "for link in links:\n",
        "    Year.append(link.get('title'))\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['tmp'] = Year\n",
        "df = df[-200:] \n",
        "year = df.iloc[::4, :]\n",
        "year['c'] = range(len(year))\n",
        "country = df.iloc[1::4, :]\n",
        "country['c'] = range(len(country))\n",
        "rider = df.iloc[2::4, :]\n",
        "rider['c'] = range(len(rider))\n",
        "team = df.iloc[3::4, :]\n",
        "team['c'] = range(len(team))\n",
        "\n",
        "# join the columns\n",
        "giro = pd.merge(year, rider, how='left', on='c')\n",
        "giro = pd.merge(giro, team, how='left', on='c')\n",
        "\n",
        "# Rename the columns\n",
        "giro.columns = ['Year', 'c', 'Rider','Team']\n",
        "giro = pd.DataFrame(giro)\n",
        "\n",
        "# Drop the 'C' column\n",
        "giro.drop(['c'], axis=1, inplace=True)\n",
        "\n",
        "# remove (cyclist) from names\n",
        "giro['Rider'] = giro['Rider'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "giro['Team'] = giro['Team'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "\n",
        "# add a binary variable for the winners\n",
        "giro['giro_win'] = 1\n",
        "\n",
        "# Let's only do the last 8 years since it gets all messed up\n",
        "giro = giro.tail(8)\n",
        "\n",
        "# save the file\n",
        "#san_remo.to_csv('san_remo.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL95t2loQLiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vuelta\n",
        "url = 'https://en.wikipedia.org/wiki/List_of_Vuelta_a_Espa%C3%B1a_general_classification_winners'\n",
        "html = requests.get(url).text\n",
        "soup = BeautifulSoup(html, 'html5lib')\n",
        "tables = soup.find('table', class_='sortable')\n",
        "links = tables.find_all('a')\n",
        "\n",
        "Year = []\n",
        "for link in links:\n",
        "    Year.append(link.get('title'))\n",
        "Year.remove('Juan José Cobo',)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['tmp'] = Year\n",
        "df = df[-200:] \n",
        "year = df.iloc[::4, :]\n",
        "year['c'] = range(len(year))\n",
        "country = df.iloc[1::4, :]\n",
        "country['c'] = range(len(country))\n",
        "rider = df.iloc[2::4, :]\n",
        "rider['c'] = range(len(rider))\n",
        "team = df.iloc[3::4, :]\n",
        "team['c'] = range(len(team))\n",
        "\n",
        "# join the columns\n",
        "vuelta = pd.merge(year, rider, how='left', on='c')\n",
        "vuelta = pd.merge(vuelta, team, how='left', on='c')\n",
        "\n",
        "# Rename the columns\n",
        "vuelta.columns = ['Year', 'c', 'Rider','Team']\n",
        "vuelta = pd.DataFrame(vuelta)\n",
        "\n",
        "# Drop the 'C' column\n",
        "vuelta.drop(['c'], axis=1, inplace=True)\n",
        "\n",
        "# remove (cyclist) from names\n",
        "vuelta['Rider'] = vuelta['Rider'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "vuelta['Team'] = vuelta['Team'].str.replace(r\"\\(.*\\)\",\"\")\n",
        "\n",
        "# add a binary variable for the winners\n",
        "vuelta['vuelta_win'] = 1\n",
        "\n",
        "# Let's only do the last 8 years since it gets all messed up\n",
        "vuelta = vuelta.tail(15)\n",
        "\n",
        "# save the file\n",
        "#san_remo.to_csv('san_remo.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEubvzeOGyPz",
        "colab_type": "text"
      },
      "source": [
        "All GC Consolidated in one table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSzrVHPrGCC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Scrape a table from wikipedia using python. Allows for cells spanning multiple rows and/or columns. Outputs csv files for\n",
        "each table\n",
        "url: https://gist.github.com/wassname/5b10774dfcd61cdd3f28\n",
        "authors: panford, wassname, muzzled, Yossi\n",
        "license: MIT\n",
        "\"\"\"\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import codecs\n",
        "wiki = \"https://en.wikipedia.org/wiki/List_of_Grand_Tour_general_classification_winners\"\n",
        "header = {\n",
        "    'User-Agent': 'Mozilla/5.0'\n",
        "}  # Needed to prevent 403 error on Wikipedia\n",
        "page = requests.get(wiki, headers=header)\n",
        "soup = BeautifulSoup(page.content)\n",
        "\n",
        "tables = soup.findAll(\"table\", {\"class\": \"wikitable\"})\n",
        "\n",
        "# show tables\n",
        "for i, table in enumerate(tables):\n",
        "    print(\"#\"*10 + \"Table {}\".format(i) + '#'*10)\n",
        "    print(table.text[:100])\n",
        "    print('.'*80)\n",
        "print(\"#\"*80)\n",
        "\n",
        "for tn, table in enumerate(tables):\n",
        "\n",
        "    # preinit list of lists\n",
        "    rows = table.findAll(\"tr\")\n",
        "    row_lengths = [len(r.findAll(['th', 'td'])) for r in rows]\n",
        "    ncols = max(row_lengths)\n",
        "    nrows = len(rows)\n",
        "    data = []\n",
        "    for i in range(nrows):\n",
        "        rowD = []\n",
        "        for j in range(ncols):\n",
        "            rowD.append('')\n",
        "        data.append(rowD)\n",
        "\n",
        "    # process html\n",
        "    for i in range(len(rows)):\n",
        "        row = rows[i]\n",
        "        rowD = []\n",
        "        cells = row.findAll([\"td\", \"th\"])\n",
        "        for j in range(len(cells)):\n",
        "            cell = cells[j]\n",
        "\n",
        "            #lots of cells span cols and rows so lets deal with that\n",
        "            cspan = int(cell.get('colspan', 1))\n",
        "            rspan = int(cell.get('rowspan', 1))\n",
        "            l = 0\n",
        "            for k in range(rspan):\n",
        "                # Shifts to the first empty cell of this row\n",
        "                while data[i + k][j + l]:\n",
        "                    l += 1\n",
        "                for m in range(cspan):\n",
        "                    cell_n = j + l + m\n",
        "                    row_n = i + k\n",
        "                    # in some cases the colspan can overflow the table, in those cases just get the last item\n",
        "                    cell_n = min(cell_n, len(data[row_n])-1)\n",
        "                    data[row_n][cell_n] += cell.text\n",
        "                    print(cell.text)\n",
        "\n",
        "        data.append(rowD)\n",
        "\n",
        "    # write data out to tab seperated format\n",
        "    page = os.path.split(wiki)[1]\n",
        "    fname = 'output_{}_t{}.tsv'.format(page, tn)\n",
        "    f = codecs.open(fname, 'w')\n",
        "    for i in range(nrows):\n",
        "        rowStr = '\\t'.join(data[i])\n",
        "        rowStr = rowStr.replace('\\n', '')\n",
        "        print(rowStr)\n",
        "        f.write(rowStr + '\\n')\n",
        "\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc3tSvLMQAlI",
        "colab_type": "text"
      },
      "source": [
        "# Non-Monument Classics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXqdiud5QVZM",
        "colab_type": "text"
      },
      "source": [
        "Strade Bianche"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnoH50PQQOmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Scrape a table from wikipedia using python. Allows for cells spanning multiple rows and/or columns. Outputs csv files for\n",
        "each table\n",
        "url: https://gist.github.com/wassname/5b10774dfcd61cdd3f28\n",
        "authors: panford, wassname, muzzled, Yossi\n",
        "license: MIT\n",
        "\"\"\"\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import codecs\n",
        "wiki = \"https://en.wikipedia.org/wiki/Strade_Bianche#Winners\"\n",
        "header = {\n",
        "    'User-Agent': 'Mozilla/5.0'\n",
        "}  # Needed to prevent 403 error on Wikipedia\n",
        "page = requests.get(wiki, headers=header)\n",
        "soup = BeautifulSoup(page.content)\n",
        "\n",
        "tables = soup.findAll(\"table\", {\"class\": \"wikitable\"})\n",
        "\n",
        "# show tables\n",
        "for i, table in enumerate(tables):\n",
        "    print(\"#\"*10 + \"Table {}\".format(i) + '#'*10)\n",
        "    print(table.text[:100])\n",
        "    print('.'*80)\n",
        "print(\"#\"*80)\n",
        "\n",
        "for tn, table in enumerate(tables):\n",
        "\n",
        "    # preinit list of lists\n",
        "    rows = table.findAll(\"tr\")\n",
        "    row_lengths = [len(r.findAll(['th', 'td'])) for r in rows]\n",
        "    ncols = max(row_lengths)\n",
        "    nrows = len(rows)\n",
        "    data = []\n",
        "    for i in range(nrows):\n",
        "        rowD = []\n",
        "        for j in range(ncols):\n",
        "            rowD.append('')\n",
        "        data.append(rowD)\n",
        "\n",
        "    # process html\n",
        "    for i in range(len(rows)):\n",
        "        row = rows[i]\n",
        "        rowD = []\n",
        "        cells = row.findAll([\"td\", \"th\"])\n",
        "        for j in range(len(cells)):\n",
        "            cell = cells[j]\n",
        "\n",
        "            #lots of cells span cols and rows so lets deal with that\n",
        "            cspan = int(cell.get('colspan', 1))\n",
        "            rspan = int(cell.get('rowspan', 1))\n",
        "            l = 0\n",
        "            for k in range(rspan):\n",
        "                # Shifts to the first empty cell of this row\n",
        "                while data[i + k][j + l]:\n",
        "                    l += 1\n",
        "                for m in range(cspan):\n",
        "                    cell_n = j + l + m\n",
        "                    row_n = i + k\n",
        "                    # in some cases the colspan can overflow the table, in those cases just get the last item\n",
        "                    cell_n = min(cell_n, len(data[row_n])-1)\n",
        "                    data[row_n][cell_n] += cell.text\n",
        "                    print(cell.text)\n",
        "\n",
        "        data.append(rowD)\n",
        "\n",
        "    # write data out to tab seperated format\n",
        "    page = os.path.split(wiki)[1]\n",
        "    fname = 'output_{}_t{}.tsv'.format(page, tn)\n",
        "    f = codecs.open(fname, 'w')\n",
        "    for i in range(nrows):\n",
        "        rowStr = '\\t'.join(data[i])\n",
        "        rowStr = rowStr.replace('\\n', '')\n",
        "        print(rowStr)\n",
        "        f.write(rowStr + '\\n')\n",
        "\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpPUP_UKRQUe",
        "colab_type": "text"
      },
      "source": [
        "E3 BinckBank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAUEsjsqRTgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Scrape a table from wikipedia using python. Allows for cells spanning multiple rows and/or columns. Outputs csv files for\n",
        "each table\n",
        "url: https://gist.github.com/wassname/5b10774dfcd61cdd3f28\n",
        "authors: panford, wassname, muzzled, Yossi\n",
        "license: MIT\n",
        "\"\"\"\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import codecs\n",
        "wiki = \"https://en.wikipedia.org/wiki/E3_BinckBank_Classic\"\n",
        "header = {\n",
        "    'User-Agent': 'Mozilla/5.0'\n",
        "}  # Needed to prevent 403 error on Wikipedia\n",
        "page = requests.get(wiki, headers=header)\n",
        "soup = BeautifulSoup(page.content)\n",
        "\n",
        "tables = soup.findAll(\"table\", {\"class\": \"wikitable\"})\n",
        "\n",
        "# show tables\n",
        "for i, table in enumerate(tables):\n",
        "    print(\"#\"*10 + \"Table {}\".format(i) + '#'*10)\n",
        "    print(table.text[:100])\n",
        "    print('.'*80)\n",
        "print(\"#\"*80)\n",
        "\n",
        "for tn, table in enumerate(tables):\n",
        "\n",
        "    # preinit list of lists\n",
        "    rows = table.findAll(\"tr\")\n",
        "    row_lengths = [len(r.findAll(['th', 'td'])) for r in rows]\n",
        "    ncols = max(row_lengths)\n",
        "    nrows = len(rows)\n",
        "    data = []\n",
        "    for i in range(nrows):\n",
        "        rowD = []\n",
        "        for j in range(ncols):\n",
        "            rowD.append('')\n",
        "        data.append(rowD)\n",
        "\n",
        "    # process html\n",
        "    for i in range(len(rows)):\n",
        "        row = rows[i]\n",
        "        rowD = []\n",
        "        cells = row.findAll([\"td\", \"th\"])\n",
        "        for j in range(len(cells)):\n",
        "            cell = cells[j]\n",
        "\n",
        "            #lots of cells span cols and rows so lets deal with that\n",
        "            cspan = int(cell.get('colspan', 1))\n",
        "            rspan = int(cell.get('rowspan', 1))\n",
        "            l = 0\n",
        "            for k in range(rspan):\n",
        "                # Shifts to the first empty cell of this row\n",
        "                while data[i + k][j + l]:\n",
        "                    l += 1\n",
        "                for m in range(cspan):\n",
        "                    cell_n = j + l + m\n",
        "                    row_n = i + k\n",
        "                    # in some cases the colspan can overflow the table, in those cases just get the last item\n",
        "                    cell_n = min(cell_n, len(data[row_n])-1)\n",
        "                    data[row_n][cell_n] += cell.text\n",
        "                    print(cell.text)\n",
        "\n",
        "        data.append(rowD)\n",
        "\n",
        "    # write data out to tab seperated format\n",
        "    page = os.path.split(wiki)[1]\n",
        "    fname = 'output_{}_t{}.tsv'.format(page, tn)\n",
        "    f = codecs.open(fname, 'w')\n",
        "    for i in range(nrows):\n",
        "        rowStr = '\\t'.join(data[i])\n",
        "        rowStr = rowStr.replace('\\n', '')\n",
        "        print(rowStr)\n",
        "        f.write(rowStr + '\\n')\n",
        "\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-r0_724SskD",
        "colab_type": "text"
      },
      "source": [
        "Gent-Wevelgem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5vF3zs_Sx17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import codecs\n",
        "wiki = \"https://en.wikipedia.org/wiki/Gent%E2%80%93Wevelgem\"\n",
        "header = {\n",
        "    'User-Agent': 'Mozilla/5.0'\n",
        "}  # Needed to prevent 403 error on Wikipedia\n",
        "page = requests.get(wiki, headers=header)\n",
        "soup = BeautifulSoup(page.content)\n",
        "\n",
        "tables = soup.findAll(\"table\", {\"class\": \"wikitable\"})\n",
        "\n",
        "# show tables\n",
        "for i, table in enumerate(tables):\n",
        "    print(\"#\"*10 + \"Table {}\".format(i) + '#'*10)\n",
        "    print(table.text[:100])\n",
        "    print('.'*80)\n",
        "print(\"#\"*80)\n",
        "\n",
        "for tn, table in enumerate(tables):\n",
        "\n",
        "    # preinit list of lists\n",
        "    rows = table.findAll(\"tr\")\n",
        "    row_lengths = [len(r.findAll(['th', 'td'])) for r in rows]\n",
        "    ncols = max(row_lengths)\n",
        "    nrows = len(rows)\n",
        "    data = []\n",
        "    for i in range(nrows):\n",
        "        rowD = []\n",
        "        for j in range(ncols):\n",
        "            rowD.append('')\n",
        "        data.append(rowD)\n",
        "\n",
        "    # process html\n",
        "    for i in range(len(rows)):\n",
        "        row = rows[i]\n",
        "        rowD = []\n",
        "        cells = row.findAll([\"td\", \"th\"])\n",
        "        for j in range(len(cells)):\n",
        "            cell = cells[j]\n",
        "\n",
        "            #lots of cells span cols and rows so lets deal with that\n",
        "            cspan = int(cell.get('colspan', 1))\n",
        "            rspan = int(cell.get('rowspan', 1))\n",
        "            l = 0\n",
        "            for k in range(rspan):\n",
        "                # Shifts to the first empty cell of this row\n",
        "                while data[i + k][j + l]:\n",
        "                    l += 1\n",
        "                for m in range(cspan):\n",
        "                    cell_n = j + l + m\n",
        "                    row_n = i + k\n",
        "                    # in some cases the colspan can overflow the table, in those cases just get the last item\n",
        "                    cell_n = min(cell_n, len(data[row_n])-1)\n",
        "                    data[row_n][cell_n] += cell.text\n",
        "                    print(cell.text)\n",
        "\n",
        "        data.append(rowD)\n",
        "\n",
        "    # write data out to tab seperated format\n",
        "    page = os.path.split(wiki)[1]\n",
        "    fname = 'output_{}_t{}.tsv'.format(page, tn)\n",
        "    f = codecs.open(fname, 'w')\n",
        "    for i in range(nrows):\n",
        "        rowStr = '\\t'.join(data[i])\n",
        "        rowStr = rowStr.replace('\\n', '')\n",
        "        print(rowStr)\n",
        "        f.write(rowStr + '\\n')\n",
        "\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZsaDVgaTR9J",
        "colab_type": "text"
      },
      "source": [
        "Amstel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-Ox59o1TS-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import codecs\n",
        "wiki = \"https://en.wikipedia.org/wiki/Amstel_Gold_Race\"\n",
        "header = {\n",
        "    'User-Agent': 'Mozilla/5.0'\n",
        "}  # Needed to prevent 403 error on Wikipedia\n",
        "page = requests.get(wiki, headers=header)\n",
        "soup = BeautifulSoup(page.content)\n",
        "\n",
        "tables = soup.findAll(\"table\", {\"class\": \"wikitable\"})\n",
        "\n",
        "# show tables\n",
        "for i, table in enumerate(tables):\n",
        "    print(\"#\"*10 + \"Table {}\".format(i) + '#'*10)\n",
        "    print(table.text[:100])\n",
        "    print('.'*80)\n",
        "print(\"#\"*80)\n",
        "\n",
        "for tn, table in enumerate(tables):\n",
        "\n",
        "    # preinit list of lists\n",
        "    rows = table.findAll(\"tr\")\n",
        "    row_lengths = [len(r.findAll(['th', 'td'])) for r in rows]\n",
        "    ncols = max(row_lengths)\n",
        "    nrows = len(rows)\n",
        "    data = []\n",
        "    for i in range(nrows):\n",
        "        rowD = []\n",
        "        for j in range(ncols):\n",
        "            rowD.append('')\n",
        "        data.append(rowD)\n",
        "\n",
        "    # process html\n",
        "    for i in range(len(rows)):\n",
        "        row = rows[i]\n",
        "        rowD = []\n",
        "        cells = row.findAll([\"td\", \"th\"])\n",
        "        for j in range(len(cells)):\n",
        "            cell = cells[j]\n",
        "\n",
        "            #lots of cells span cols and rows so lets deal with that\n",
        "            cspan = int(cell.get('colspan', 1))\n",
        "            rspan = int(cell.get('rowspan', 1))\n",
        "            l = 0\n",
        "            for k in range(rspan):\n",
        "                # Shifts to the first empty cell of this row\n",
        "                while data[i + k][j + l]:\n",
        "                    l += 1\n",
        "                for m in range(cspan):\n",
        "                    cell_n = j + l + m\n",
        "                    row_n = i + k\n",
        "                    # in some cases the colspan can overflow the table, in those cases just get the last item\n",
        "                    cell_n = min(cell_n, len(data[row_n])-1)\n",
        "                    data[row_n][cell_n] += cell.text\n",
        "                    print(cell.text)\n",
        "\n",
        "        data.append(rowD)\n",
        "\n",
        "    # write data out to tab seperated format\n",
        "    page = os.path.split(wiki)[1]\n",
        "    fname = 'output_{}_t{}.tsv'.format(page, tn)\n",
        "    f = codecs.open(fname, 'w')\n",
        "    for i in range(nrows):\n",
        "        rowStr = '\\t'.join(data[i])\n",
        "        rowStr = rowStr.replace('\\n', '')\n",
        "        print(rowStr)\n",
        "        f.write(rowStr + '\\n')\n",
        "\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7yjizp6UHqN",
        "colab_type": "text"
      },
      "source": [
        "Fleche"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhZzeJy5UJEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import codecs\n",
        "wiki = \"https://en.wikipedia.org/wiki/La_Fl%C3%A8che_Wallonne\"\n",
        "header = {\n",
        "    'User-Agent': 'Mozilla/5.0'\n",
        "}  # Needed to prevent 403 error on Wikipedia\n",
        "page = requests.get(wiki, headers=header)\n",
        "soup = BeautifulSoup(page.content)\n",
        "\n",
        "tables = soup.findAll(\"table\", {\"class\": \"wikitable\"})\n",
        "\n",
        "# show tables\n",
        "for i, table in enumerate(tables):\n",
        "    print(\"#\"*10 + \"Table {}\".format(i) + '#'*10)\n",
        "    print(table.text[:100])\n",
        "    print('.'*80)\n",
        "print(\"#\"*80)\n",
        "\n",
        "for tn, table in enumerate(tables):\n",
        "\n",
        "    # preinit list of lists\n",
        "    rows = table.findAll(\"tr\")\n",
        "    row_lengths = [len(r.findAll(['th', 'td'])) for r in rows]\n",
        "    ncols = max(row_lengths)\n",
        "    nrows = len(rows)\n",
        "    data = []\n",
        "    for i in range(nrows):\n",
        "        rowD = []\n",
        "        for j in range(ncols):\n",
        "            rowD.append('')\n",
        "        data.append(rowD)\n",
        "\n",
        "    # process html\n",
        "    for i in range(len(rows)):\n",
        "        row = rows[i]\n",
        "        rowD = []\n",
        "        cells = row.findAll([\"td\", \"th\"])\n",
        "        for j in range(len(cells)):\n",
        "            cell = cells[j]\n",
        "\n",
        "            #lots of cells span cols and rows so lets deal with that\n",
        "            cspan = int(cell.get('colspan', 1))\n",
        "            rspan = int(cell.get('rowspan', 1))\n",
        "            l = 0\n",
        "            for k in range(rspan):\n",
        "                # Shifts to the first empty cell of this row\n",
        "                while data[i + k][j + l]:\n",
        "                    l += 1\n",
        "                for m in range(cspan):\n",
        "                    cell_n = j + l + m\n",
        "                    row_n = i + k\n",
        "                    # in some cases the colspan can overflow the table, in those cases just get the last item\n",
        "                    cell_n = min(cell_n, len(data[row_n])-1)\n",
        "                    data[row_n][cell_n] += cell.text\n",
        "                    print(cell.text)\n",
        "\n",
        "        data.append(rowD)\n",
        "\n",
        "    # write data out to tab seperated format\n",
        "    page = os.path.split(wiki)[1]\n",
        "    fname = 'output_{}_t{}.tsv'.format(page, tn)\n",
        "    f = codecs.open(fname, 'w')\n",
        "    for i in range(nrows):\n",
        "        rowStr = '\\t'.join(data[i])\n",
        "        rowStr = rowStr.replace('\\n', '')\n",
        "        print(rowStr)\n",
        "        f.write(rowStr + '\\n')\n",
        "\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhAkW07kVxgf",
        "colab_type": "text"
      },
      "source": [
        "# Smaller Tours or GC warm ups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRXM8CumUosN",
        "colab_type": "text"
      },
      "source": [
        "Tirreno is a bit weird, will need to "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2tOyr2FV2oi",
        "colab_type": "text"
      },
      "source": [
        "Critérium_du_Dauphiné"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDQcJQZRV7t0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import codecs\n",
        "wiki = \"https://en.wikipedia.org/wiki/Crit%C3%A9rium_du_Dauphin%C3%A9\"\n",
        "header = {\n",
        "    'User-Agent': 'Mozilla/5.0'\n",
        "}  # Needed to prevent 403 error on Wikipedia\n",
        "page = requests.get(wiki, headers=header)\n",
        "soup = BeautifulSoup(page.content)\n",
        "\n",
        "tables = soup.findAll(\"table\", {\"class\": \"wikitable\"})\n",
        "\n",
        "# show tables\n",
        "for i, table in enumerate(tables):\n",
        "    print(\"#\"*10 + \"Table {}\".format(i) + '#'*10)\n",
        "    print(table.text[:100])\n",
        "    print('.'*80)\n",
        "print(\"#\"*80)\n",
        "\n",
        "for tn, table in enumerate(tables):\n",
        "\n",
        "    # preinit list of lists\n",
        "    rows = table.findAll(\"tr\")\n",
        "    row_lengths = [len(r.findAll(['th', 'td'])) for r in rows]\n",
        "    ncols = max(row_lengths)\n",
        "    nrows = len(rows)\n",
        "    data = []\n",
        "    for i in range(nrows):\n",
        "        rowD = []\n",
        "        for j in range(ncols):\n",
        "            rowD.append('')\n",
        "        data.append(rowD)\n",
        "\n",
        "    # process html\n",
        "    for i in range(len(rows)):\n",
        "        row = rows[i]\n",
        "        rowD = []\n",
        "        cells = row.findAll([\"td\", \"th\"])\n",
        "        for j in range(len(cells)):\n",
        "            cell = cells[j]\n",
        "\n",
        "            #lots of cells span cols and rows so lets deal with that\n",
        "            cspan = int(cell.get('colspan', 1))\n",
        "            rspan = int(cell.get('rowspan', 1))\n",
        "            l = 0\n",
        "            for k in range(rspan):\n",
        "                # Shifts to the first empty cell of this row\n",
        "                while data[i + k][j + l]:\n",
        "                    l += 1\n",
        "                for m in range(cspan):\n",
        "                    cell_n = j + l + m\n",
        "                    row_n = i + k\n",
        "                    # in some cases the colspan can overflow the table, in those cases just get the last item\n",
        "                    cell_n = min(cell_n, len(data[row_n])-1)\n",
        "                    data[row_n][cell_n] += cell.text\n",
        "                    print(cell.text)\n",
        "\n",
        "        data.append(rowD)\n",
        "\n",
        "    # write data out to tab seperated format\n",
        "    page = os.path.split(wiki)[1]\n",
        "    fname = 'output_{}_t{}.tsv'.format(page, tn)\n",
        "    f = codecs.open(fname, 'w')\n",
        "    for i in range(nrows):\n",
        "        rowStr = '\\t'.join(data[i])\n",
        "        rowStr = rowStr.replace('\\n', '')\n",
        "        print(rowStr)\n",
        "        f.write(rowStr + '\\n')\n",
        "\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dYsxNErWbdH",
        "colab_type": "text"
      },
      "source": [
        "Tour of Romandie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA6dmWc0WeWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import codecs\n",
        "wiki = \"https://en.wikipedia.org/wiki/Tour_de_Romandie\"\n",
        "header = {\n",
        "    'User-Agent': 'Mozilla/5.0'\n",
        "}  # Needed to prevent 403 error on Wikipedia\n",
        "page = requests.get(wiki, headers=header)\n",
        "soup = BeautifulSoup(page.content)\n",
        "\n",
        "tables = soup.findAll(\"table\", {\"class\": \"wikitable\"})\n",
        "\n",
        "# show tables\n",
        "for i, table in enumerate(tables):\n",
        "    print(\"#\"*10 + \"Table {}\".format(i) + '#'*10)\n",
        "    print(table.text[:100])\n",
        "    print('.'*80)\n",
        "print(\"#\"*80)\n",
        "\n",
        "for tn, table in enumerate(tables):\n",
        "\n",
        "    # preinit list of lists\n",
        "    rows = table.findAll(\"tr\")\n",
        "    row_lengths = [len(r.findAll(['th', 'td'])) for r in rows]\n",
        "    ncols = max(row_lengths)\n",
        "    nrows = len(rows)\n",
        "    data = []\n",
        "    for i in range(nrows):\n",
        "        rowD = []\n",
        "        for j in range(ncols):\n",
        "            rowD.append('')\n",
        "        data.append(rowD)\n",
        "\n",
        "    # process html\n",
        "    for i in range(len(rows)):\n",
        "        row = rows[i]\n",
        "        rowD = []\n",
        "        cells = row.findAll([\"td\", \"th\"])\n",
        "        for j in range(len(cells)):\n",
        "            cell = cells[j]\n",
        "\n",
        "            #lots of cells span cols and rows so lets deal with that\n",
        "            cspan = int(cell.get('colspan', 1))\n",
        "            rspan = int(cell.get('rowspan', 1))\n",
        "            l = 0\n",
        "            for k in range(rspan):\n",
        "                # Shifts to the first empty cell of this row\n",
        "                while data[i + k][j + l]:\n",
        "                    l += 1\n",
        "                for m in range(cspan):\n",
        "                    cell_n = j + l + m\n",
        "                    row_n = i + k\n",
        "                    # in some cases the colspan can overflow the table, in those cases just get the last item\n",
        "                    cell_n = min(cell_n, len(data[row_n])-1)\n",
        "                    data[row_n][cell_n] += cell.text\n",
        "                    print(cell.text)\n",
        "\n",
        "        data.append(rowD)\n",
        "\n",
        "    # write data out to tab seperated format\n",
        "    page = os.path.split(wiki)[1]\n",
        "    fname = 'output_{}_t{}.tsv'.format(page, tn)\n",
        "    f = codecs.open(fname, 'w')\n",
        "    for i in range(nrows):\n",
        "        rowStr = '\\t'.join(data[i])\n",
        "        rowStr = rowStr.replace('\\n', '')\n",
        "        print(rowStr)\n",
        "        f.write(rowStr + '\\n')\n",
        "\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL0pwf5CWzeO",
        "colab_type": "text"
      },
      "source": [
        "Tour de Suisse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIPW__dvW1JO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import codecs\n",
        "wiki = \"https://en.wikipedia.org/wiki/Tour_de_Suisse\"\n",
        "header = {\n",
        "    'User-Agent': 'Mozilla/5.0'\n",
        "}  # Needed to prevent 403 error on Wikipedia\n",
        "page = requests.get(wiki, headers=header)\n",
        "soup = BeautifulSoup(page.content)\n",
        "\n",
        "tables = soup.findAll(\"table\", {\"class\": \"wikitable\"})\n",
        "\n",
        "# show tables\n",
        "for i, table in enumerate(tables):\n",
        "    print(\"#\"*10 + \"Table {}\".format(i) + '#'*10)\n",
        "    print(table.text[:100])\n",
        "    print('.'*80)\n",
        "print(\"#\"*80)\n",
        "\n",
        "for tn, table in enumerate(tables):\n",
        "\n",
        "    # preinit list of lists\n",
        "    rows = table.findAll(\"tr\")\n",
        "    row_lengths = [len(r.findAll(['th', 'td'])) for r in rows]\n",
        "    ncols = max(row_lengths)\n",
        "    nrows = len(rows)\n",
        "    data = []\n",
        "    for i in range(nrows):\n",
        "        rowD = []\n",
        "        for j in range(ncols):\n",
        "            rowD.append('')\n",
        "        data.append(rowD)\n",
        "\n",
        "    # process html\n",
        "    for i in range(len(rows)):\n",
        "        row = rows[i]\n",
        "        rowD = []\n",
        "        cells = row.findAll([\"td\", \"th\"])\n",
        "        for j in range(len(cells)):\n",
        "            cell = cells[j]\n",
        "\n",
        "            #lots of cells span cols and rows so lets deal with that\n",
        "            cspan = int(cell.get('colspan', 1))\n",
        "            rspan = int(cell.get('rowspan', 1))\n",
        "            l = 0\n",
        "            for k in range(rspan):\n",
        "                # Shifts to the first empty cell of this row\n",
        "                while data[i + k][j + l]:\n",
        "                    l += 1\n",
        "                for m in range(cspan):\n",
        "                    cell_n = j + l + m\n",
        "                    row_n = i + k\n",
        "                    # in some cases the colspan can overflow the table, in those cases just get the last item\n",
        "                    cell_n = min(cell_n, len(data[row_n])-1)\n",
        "                    data[row_n][cell_n] += cell.text\n",
        "                    print(cell.text)\n",
        "\n",
        "        data.append(rowD)\n",
        "\n",
        "    # write data out to tab seperated format\n",
        "    page = os.path.split(wiki)[1]\n",
        "    fname = 'output_{}_t{}.tsv'.format(page, tn)\n",
        "    f = codecs.open(fname, 'w')\n",
        "    for i in range(nrows):\n",
        "        rowStr = '\\t'.join(data[i])\n",
        "        rowStr = rowStr.replace('\\n', '')\n",
        "        print(rowStr)\n",
        "        f.write(rowStr + '\\n')\n",
        "\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM2aaHP9XRoe",
        "colab_type": "text"
      },
      "source": [
        "Paris-Nice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRxlmVxEXWWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import codecs\n",
        "wiki = \"https://en.wikipedia.org/wiki/Paris%E2%80%93Nice\"\n",
        "header = {\n",
        "    'User-Agent': 'Mozilla/5.0'\n",
        "}  # Needed to prevent 403 error on Wikipedia\n",
        "page = requests.get(wiki, headers=header)\n",
        "soup = BeautifulSoup(page.content)\n",
        "\n",
        "tables = soup.findAll(\"table\", {\"class\": \"wikitable\"})\n",
        "\n",
        "# show tables\n",
        "for i, table in enumerate(tables):\n",
        "    print(\"#\"*10 + \"Table {}\".format(i) + '#'*10)\n",
        "    print(table.text[:100])\n",
        "    print('.'*80)\n",
        "print(\"#\"*80)\n",
        "\n",
        "for tn, table in enumerate(tables):\n",
        "\n",
        "    # preinit list of lists\n",
        "    rows = table.findAll(\"tr\")\n",
        "    row_lengths = [len(r.findAll(['th', 'td'])) for r in rows]\n",
        "    ncols = max(row_lengths)\n",
        "    nrows = len(rows)\n",
        "    data = []\n",
        "    for i in range(nrows):\n",
        "        rowD = []\n",
        "        for j in range(ncols):\n",
        "            rowD.append('')\n",
        "        data.append(rowD)\n",
        "\n",
        "    # process html\n",
        "    for i in range(len(rows)):\n",
        "        row = rows[i]\n",
        "        rowD = []\n",
        "        cells = row.findAll([\"td\", \"th\"])\n",
        "        for j in range(len(cells)):\n",
        "            cell = cells[j]\n",
        "\n",
        "            #lots of cells span cols and rows so lets deal with that\n",
        "            cspan = int(cell.get('colspan', 1))\n",
        "            rspan = int(cell.get('rowspan', 1))\n",
        "            l = 0\n",
        "            for k in range(rspan):\n",
        "                # Shifts to the first empty cell of this row\n",
        "                while data[i + k][j + l]:\n",
        "                    l += 1\n",
        "                for m in range(cspan):\n",
        "                    cell_n = j + l + m\n",
        "                    row_n = i + k\n",
        "                    # in some cases the colspan can overflow the table, in those cases just get the last item\n",
        "                    cell_n = min(cell_n, len(data[row_n])-1)\n",
        "                    data[row_n][cell_n] += cell.text\n",
        "                    print(cell.text)\n",
        "\n",
        "        data.append(rowD)\n",
        "\n",
        "    # write data out to tab seperated format\n",
        "    page = os.path.split(wiki)[1]\n",
        "    fname = 'output_{}_t{}.tsv'.format(page, tn)\n",
        "    f = codecs.open(fname, 'w')\n",
        "    for i in range(nrows):\n",
        "        rowStr = '\\t'.join(data[i])\n",
        "        rowStr = rowStr.replace('\\n', '')\n",
        "        print(rowStr)\n",
        "        f.write(rowStr + '\\n')\n",
        "\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}